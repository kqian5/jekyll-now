---
layout: post
title: The Dystopian Future Nobody Wants
---
Machine learning is hot hot hot right now, yes even more so than Drake’s newest album, and it was simply a matter of time before the camera industry hopped on that bandwagon. Machine learning is in essence a subset of Artificial Intelligence that allow computers to learn from data without being explicitly programmed. Pair that up with cameras and we suddenly have cameras that ostensibly see like humans.

We see it in Apple’s newest iPhone X, in Facebook’s tagging suggestions, and countless other products. Google recently released Google Clips, an extraordinary AI powered camera that seems to accomplish many of the inhuman wonders expected of today’s machines. It uses machine learning techniques to automatically capture whatever it deems “interesting”, by responding to lighting, facial expressions, and other aspects of a good photograph. This product is truly revolutionary and is meant to transform the camera/video/entertainment industry. Since beauty is a very abstract concept, it is incredible that we can possibly program a machine to understand a concept that we have difficulty describing. To use the camera, you simply turn it on and leave it pointing in the general target vicinity; all of the photography happens automatically. Therefore it can easily capture tons of organic and candid photos without much human effort, and even allows people to momentarily forget about their dreadful fear of the filming process! No longer will parents have to force their children to act natural in front of an eeringly weapon-like object shaped like a camera. No longer will children have to worry about if they are awkwardly smiling when adults tell them to “Smile!” Furthermore, all of the data from the camera is stored locally, so privacy is protected and ensured against harmful agents like the government. Ha. I admit I have quite lofty expectations for this camera and am concerned about my expectations not being met. Like, all photos of me MUST be good looking because I will accept nothing less than the truthful depiction of my photogenic self. But I am also sure that there are far greater worries to fret about... 

![GoogleClips](/images/GoogleClips.jpg){:class="img-responsive" height="366px" width="550px"}

Moving on to a more serious note, a major use for these smart-cameras is in security and surveillance. Tech companies are developing surveillance camera software that can recognize faces and the dangerous behaviors performed by the owners of those faces. This could help reduce crime rates by identifying criminals and other lawbreakers. Even the perception of smart surveillance, even if it isn’t actually functional, can work to deter crimes. This innovative technology is exciting and has multitudes of applications and potential, but at what cost?

![SurveillanceGif](/images/ai_china_surveillance.gif){:class="img-responsive"}

Privacy issues raise an obvious concern. People will without a doubt oppose being captured on camera without their consent. Information gleaned from video surveillance is extremely valuable and can be used to manipulate people, whether for personalized advertisements or worse: malicious intents. One can easily learn about a person’s habits, location and other important personal information. Just look at all of the lawsuits with Facebook recently over their facial recognition and tagging feature to see the outrage that people demonstrate over privacy violations. Tech companies have tried to bring more transparency towards their data-collecting methods but this in itself is not a solution to the privacy issue. Allowing users to toggle these invasive features does not aptly count either. What needs to happen is some form of valuation over the worth of data and appropriate compensation so that corporate forces no longer exploit our information in the process of building their profit-inflating products. But that is a topic for another day.

A less visible risk includes potential bias towards targeted groups of people by the smart-cameras. These cameras are trained on data-sets, and if these data-sets are biased then the cameras will inherit this behavior. For example, if a machine learns from recordings that show police officers exhibiting racial or sexist prejudices, it will for sure soak up these behaviors. And this sort of bias can lead to a wrongful conviction or far worse. This is a sensitive topic for all areas of machine learning and in the age of big data, it is exceedingly important that we choose and interpret our data appropriately. 

China has been investing in this very heavily and plans to install a mass surveillance system that can ultimately be used to control its citizens. Imagine a dystopian society where the government watches your every move and secretly removes all traces of opposition. Now magnify that level of frightening many times over and that will be China in the future, since real life is of course scarier than fiction. Their goal has been quoted to be “algorithmic dominance”, which is as alarming as it sounds. It is clear why this sort of technology is so dangerous and why regulation over this technology remains crucial. 

![SurveillanceCamera](/images/Surveillance.jpg){:class="img-responsive" height="404px" width="498px"}

In some parts of China, this fear has already become palpable. The Uyghur minority in Xinjiang, China have been facing high-tech surveillance and monitoring technology since their riots in 2009. From social media monitoring to dna samples to now facial recognition tracking software, the Islamic Uyghur population are involuntarily forced to participate in religious re-education systems and every moment of their life is controlled. According to The New York Times, "When Uighurs buy a kitchen knife, their ID data is etched on the blade as a QR code." BuzzFeed documented stories of family members too scared to speak openly to relatives abroad. As a result, riots, protests, and violence have fallen in Xinjiang thanks to the fear instilled by the government. This was the first sign of a large-scale shift in tactics by the Chinese using a powerful tool: technological control over information. Inspired by their “success”, China now aims to implement over its entire, humongous landmass. Unfortunately, it is not just China that is determined to invest in video/camera and AI. Western powers --companies and governments-- seek this control over data as well, to a lesser extent.

To me, effective regulation over video surveillance/facial recognition is a grandiose ambition, perhaps even too ambitious. In the current world, dominated by those with capitalist agendas and with the data of the many in the hands of the few, it may be all but possible to achieve this state of equilibrium between continued innovation and user protection. Still, I remain optimistic that we can come close to a solution. I am only focusing on the US for now, only because it is the most relevant to me. I argue that we must first start with setting the right tone towards this technology in the government. For, the government is in control of regulating itself and the businesses involved with this technology. This can be done by introducing laws that increase transparency, give users more control, raise awareness, and protect privacy. We must choose the right people for this job and perhaps a special committee is needed to further investigate these matters. Currently, the Federal Trade Commission is in charge of protecting user privacy, but it is unclear if they are right for the job. This is because the main purpose of the FTC is to deal with “unfair and deceptive” business practices rather than deal with specific privacy concerns. So, they may be hesitant in exercising power over issues that are not outright illegal, which many privacy matters fall into. 

Another reason for government intervention is that we cannot rely on the tech industry to choose the protection of users over profits. A common attitude in the tech industry towards abuse of technologies is that the technologies themselves are value neutral, and that it is how they are implemented and by whom that determines if they are good or bad. In other words, they refuse to take responsibility. 

The future remains bright in the face of the AI revolution and the spread of AI into camera/video. But there are many issues concerning this rapid innovation that may cast a shadow on the brightness. However with an appropriate and unified approach, we can mitigate these shadows.
